{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from pathlib import Path\n",
    "# import scipy                # scipy.signal.spectrogram is the squared mag of stft of a signal \n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convenience functions\n",
    "\n",
    "Maybe I should normalise the data, but cba atm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"./AudioWAV\"\n",
    "\n",
    "def audio_to_flattened_spectrogram(file_path, max_pad_len):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "    S_DB = librosa.amplitude_to_db(S, ref=np.max)\n",
    "    padded_S_DB = np.pad(S_DB, ((0, 0), (0, max_pad_len - S_DB.shape[1])), mode='constant') if S_DB.shape[1] < max_pad_len else S_DB[:, :max_pad_len]\n",
    "    return padded_S_DB.flatten()\n",
    "\n",
    "# Function to determine the max length for padding/truncating by analyzing audio data\n",
    "def determine_max_length(base_dir):\n",
    "    max_len = 0\n",
    "    for filename in os.listdir(base_dir):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            file_path = os.path.join(base_dir, filename)\n",
    "            y, sr = librosa.load(file_path)\n",
    "            S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "            if S.shape[1] > max_len:\n",
    "                max_len = S.shape[1]\n",
    "    return max_len\n",
    "\n",
    "def load_data(base_dir, max_pad_len):\n",
    "    X = []\n",
    "    y = []\n",
    "    for filename in os.listdir(base_dir):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            file_path = os.path.join(base_dir, filename)\n",
    "            if \"IN\" in filename:\n",
    "                label = 0  # Category \"IN\"\n",
    "            elif \"NOT\" in filename:\n",
    "                label = 1  # Category \"NOT\"\n",
    "            spectrogram = audio_to_flattened_spectrogram(file_path, max_pad_len)\n",
    "            X.append(spectrogram)\n",
    "            y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# maximum spectrogram length\n",
    "max_length = determine_max_length(base_dir)\n",
    "X, y = load_data(base_dir, max_length)\n",
    "\n",
    "# 60/20/20 split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42) \n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n",
    "    if train:\n",
    "        pred = clf.predict(X_train)\n",
    "        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n",
    "        print(\"Train Result:\\n================================================\")\n",
    "        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n",
    "        \n",
    "    elif train==False:\n",
    "        pred = clf.predict(X_test)\n",
    "        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n",
    "        print(\"Test Result:\\n================================================\")        \n",
    "        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 100.00%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                0       1  accuracy  macro avg  weighted avg\n",
      "precision     1.0     1.0       1.0        1.0           1.0\n",
      "recall        1.0     1.0       1.0        1.0           1.0\n",
      "f1-score      1.0     1.0       1.0        1.0           1.0\n",
      "support    1502.0  2963.0       1.0     4465.0        4465.0\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1502    0]\n",
      " [   0 2963]]\n",
      "\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 61.65%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                    0           1  accuracy    macro avg  weighted avg\n",
      "precision    0.468401    0.700315  0.616521     0.584358      0.616677\n",
      "recall       0.469274    0.699580  0.616521     0.584427      0.616521\n",
      "f1-score     0.468837    0.699947  0.616521     0.584392      0.616599\n",
      "support    537.000000  952.000000  0.616521  1489.000000   1489.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[252 285]\n",
      " [286 666]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train, y_train)\n",
    "\n",
    "# save model. I cannot currently think of another way to check sizes of classical ml models\n",
    "filename = 'lr_model.sav'\n",
    "pickle.dump(lr_clf, open(filename, 'wb'))\n",
    "\n",
    "print_score(lr_clf, X_train, y_train, X_test, y_test, train=True)\n",
    "print_score(lr_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 100.00%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                0       1  accuracy  macro avg  weighted avg\n",
      "precision     1.0     1.0       1.0        1.0           1.0\n",
      "recall        1.0     1.0       1.0        1.0           1.0\n",
      "f1-score      1.0     1.0       1.0        1.0           1.0\n",
      "support    1502.0  2963.0       1.0     4465.0        4465.0\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[1502    0]\n",
      " [   0 2963]]\n",
      "\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 61.45%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                    0           1  accuracy    macro avg  weighted avg\n",
      "precision    0.463511    0.692464  0.614506     0.577988      0.609893\n",
      "recall       0.437616    0.714286  0.614506     0.575951      0.614506\n",
      "f1-score     0.450192    0.703206  0.614506     0.576699      0.611958\n",
      "support    537.000000  952.000000  0.614506  1489.000000   1489.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[235 302]\n",
      " [272 680]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "tree_clf = tree.DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "filename = 'tree_model.sav'\n",
    "pickle.dump(tree_clf, open(filename, 'wb'))\n",
    "\n",
    "print_score(tree_clf, X_train, y_train, X_test, y_test, train=True)\n",
    "print_score(tree_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to plot decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 8))\n",
    "# tree.plot_tree(tree_clf, feature_names=list(X.columns), class_names=['No', 'Yes'], filled=True, rounded=True)\n",
    "# plt.savefig(\"decision_tree.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "This is an arbitrary model, and serves to show the size of an SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Result:\n",
      "================================================\n",
      "Accuracy Score: 66.38%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "                     0            1  accuracy    macro avg  weighted avg\n",
      "precision     1.000000     0.663754   0.66383     0.831877      0.776866\n",
      "recall        0.000666     1.000000   0.66383     0.500333      0.663830\n",
      "f1-score      0.001331     0.797900   0.66383     0.399615      0.529938\n",
      "support    1502.000000  2963.000000   0.66383  4465.000000   4465.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[   1 1501]\n",
      " [   0 2963]]\n",
      "\n",
      "Test Result:\n",
      "================================================\n",
      "Accuracy Score: 63.94%\n",
      "_______________________________________________\n",
      "CLASSIFICATION REPORT:\n",
      "               0           1  accuracy    macro avg  weighted avg\n",
      "precision    0.0    0.639355  0.639355     0.319678      0.408775\n",
      "recall       0.0    1.000000  0.639355     0.500000      0.639355\n",
      "f1-score     0.0    0.780008  0.639355     0.390004      0.498702\n",
      "support    537.0  952.000000  0.639355  1489.000000   1489.000000\n",
      "_______________________________________________\n",
      "Confusion Matrix: \n",
      " [[  0 537]\n",
      " [  0 952]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf  = SVC( C=1.0,              # regularization. This may require tuning\n",
    "               kernel='sigmoid',    # radial or gaussian kernel. Can try differnt ones esp radial\n",
    "               random_state=42,\n",
    "                gamma=0.1,\n",
    "                degree=2,           # irrelevant except for poly\n",
    "                class_weight = \"balanced\")\n",
    "\n",
    "svm_clf.fit(X_train, y_train)       # should I normalise or nah\n",
    "\n",
    "filename = 'svm_model.sav'\n",
    "pickle.dump(svm_clf, open(filename, 'wb'))\n",
    "\n",
    "print_score(svm_clf, X_train, y_train, X_test, y_test, train=True)\n",
    "print_score(svm_clf, X_train, y_train, X_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On model sizes\n",
    "\n",
    "| Model Type          | Size   |\n",
    "|---------------------|--------|\n",
    "| Logistic Regression | 217 KB |\n",
    "| Tree                |  65 KB |\n",
    "| SVM                 | 964,529 KB |\n",
    "\n",
    "As our NICLA voice only has 512KB flash, the SVM is not a viable model. The accuracy of the other two models are too low to be the optimal models, but could be used in the worst case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
